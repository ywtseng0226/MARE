# @package _global_

optimizer:
  type: AdamW
  lr: 2.0e-4
  eps: 1.0e-3  # for fp16
  weight_decay: 1.0e-4
  # paramwise_cfg:
  #   - name: backbone
  #     lr_mult: 0.1
scheduler:
  type: MultiStepLR
  milestones: [25]
  gamma: 0.1

trainer:
  max_epochs: 30
  # gradient_clip_val: 35.0
